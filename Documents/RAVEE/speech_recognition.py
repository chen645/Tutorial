# -*- coding: utf-8 -*-
"""speech_recognition

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G7ZBtunxTRcClgrgmJIqM_eG8GL4ayBP
"""

#!/usr/bin/env python
# coding: utf-8

# In[ ]:

'''
from google.colab import drive
drive.mount('/content/drive/')
'''


# In[1]:

import librosa
import keras.backend as K
import tensorflow as tf
import numpy as np

import os
from keras import Input
from keras.engine import Model
from keras.layers import Lambda
from keras.models import model_from_json
import pickle
from tensorflow.python.ops import ctc_ops as ctc
from glob import glob
import cv2
import matplotlib.pyplot as plt
import numpy as np
from tqdm.auto import tqdm
import keras
from keras.layers import Input, Dense, Conv2D, MaxPool2D, Flatten
from keras.models import Model
from sklearn.model_selection import train_test_split

from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from scipy.fftpack import dct


# In[2]:


data_path = 'true_emotion_speech/*/*.wav' 
img_paths = glob(data_path)
data_count = len(glob(data_path))
print(data_count)
class_map = {'angry':0, 'disgust':1, 'fearful':2,'happy':3,'neautral':4,'sad':5,'surprised':6}
X = np.zeros((data_count, 278, 40))
y = np.zeros((data_count, ))


# In[ ]:

'''
for i, path in tqdm(enumerate(img_paths)):
    signal,sample_rate = librosa.load(path,sr=22000,mono=True)  # File assumed to be in the same directory

    signal = signal[0:int(2.8 * sample_rate)]  # Keep the first 3.5 seconds
    pre_emphasis = 0.97
    emphasized_signal = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])
    frame_size=0.025
    frame_stride=0.01
    frame_length, frame_step = frame_size * sample_rate, frame_stride * sample_rate  # Convert from seconds to samples
    signal_length = len(emphasized_signal)
    frame_length = int(round(frame_length))
    frame_step = int(round(frame_step))
    num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))  # Make sure that we have at least 1 frame

    pad_signal_length = num_frames * frame_step + frame_length
    z = np.zeros((pad_signal_length - signal_length))
    pad_signal = np.append(emphasized_signal, z) # Pad Signal to make sure that all frames have equal number of samples without truncating any samples from the original signal

    indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T
    frames = pad_signal[indices.astype(np.int32, copy=False)]
    frames *= np.hamming(frame_length)
    NFFT=512
    mag_frames = np.absolute(np.fft.rfft(frames, NFFT))  # Magnitude of the FFT
    pow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))  # Power Spectrum
    
    nfilt = 40
    low_freq_mel = 0
    high_freq_mel = (2595 * np.log10(1 + (sample_rate / 2) / 700))  # Convert Hz to Mel
    mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)  # Equally spaced in Mel scale
    hz_points = (700 * (10**(mel_points / 2595) - 1))  # Convert Mel to Hz
    bin = np.floor((NFFT + 1) * hz_points / sample_rate)

    fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))
    for m in range(1, nfilt + 1):
        f_m_minus = int(bin[m - 1])   # left
        f_m = int(bin[m])             # center
        f_m_plus = int(bin[m + 1])    # right

        for k in range(f_m_minus, f_m):
            fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])
        for k in range(f_m, f_m_plus):
            fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])
    filter_banks = np.dot(pow_frames, fbank.T)
    filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)  # Numerical Stability
    filter_banks = 20 * np.log10(filter_banks)  # dB
    
    num_ceps=12
    mfcc = dct(filter_banks, type=2, axis=1, norm='ortho')[:, 1 : (num_ceps + 1)]
    (nframes, ncoeff) = mfcc.shape
    cep_lifter = 22
    n = np.arange(ncoeff)
    lift = 1 + (cep_lifter / 2) * np.sin(np.pi * n / cep_lifter)
    mfcc *= lift  #*
    filter_banks -= (np.mean(filter_banks, axis=0) + 1e-8)
    X[i] = filter_banks
  
    cls = path.split('/')[-2]
  
    y[i] = class_map[cls]


#In[ ]:

X = np.reshape(X,(data_count,278,40,1))
y_onehot = keras.utils.to_categorical(y, num_classes=7)
np.save('X_train2', X)
np.save('y_train2',y_onehot)
'''






from glob import glob


X = np.load('X_train2.npy')
y_onehot = np.load('y_train2.npy')
X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=5566)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
from keras.layers import TimeDistributed, Activation, Dense, Input, Bidirectional, LSTM, Masking, GaussianNoise
from keras.optimizers import Adam

import pickle
from keras.preprocessing import sequence
import numpy as np
from keras import backend as K
from keras.models import Sequential
import numpy as np
from keras.layers.convolutional import Conv3D
from keras.layers import TimeDistributed
from keras.layers import Embedding
from keras.layers import Dense, Dropout, Activation
from keras.layers.recurrent import LSTM
from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D

from keras.layers import BatchNormalization, Activation, Conv1D, MaxPooling1D, ZeroPadding1D, InputLayer
from keras.layers.core import Lambda



model = Sequential()
model.add(Conv2D(16, kernel_size=(3, 3),
                 activation='relu',
                 input_shape=(278,40,1),padding='same'))

model.add(MaxPooling2D(pool_size=(2, 2), strides=2))
model.add(Conv2D(32, kernel_size=(3, 3),
                 activation='relu',
                 padding='same'))


model.add(MaxPooling2D(pool_size=(2, 2), strides=2))
model.add(Conv2D(64, kernel_size=(3, 3),
                 activation='relu',
                 padding='same'))

model.add(MaxPooling2D(pool_size=(2, 2), strides=2))

model.add(Conv2D(128, kernel_size=(3, 3),
                 activation='relu',
                 padding='same'))

model.add(MaxPooling2D(pool_size=(2, 2), strides=2))
model.add(Conv2D(256, kernel_size=(3, 3),
                 activation='relu',
                 padding='same'))

model.add(MaxPooling2D(pool_size=(2, 2), strides=2))



model.add(Lambda(lambda x: K.squeeze(x,2)))

model.summary()
model.add(Bidirectional(LSTM(128,return_sequences=True,dropout=0.25)))
model.add(Bidirectional(LSTM(128,return_sequences=False)))
model.summary()
model.add(Dropout(0.25))

model.add(Dense(1000, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(7,activation='softmax'))  
        


model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

print('Train...')
log=model.fit(X_train, y_train,
                batch_size=16,
                epochs=1000,verbose=1,validation_data=(X_test,y_test))


#儲存訓練過程中 的 loss and acc

model.save('ASR1.h5')  # creates a HDF5 file 'model.h5'


# In[ ]:

# In[ ]:



from sklearn.metrics import classification_report, confusion_matrix



import os
data_path1 = 'true_emotion_speech_test/*/*.wav' 
img_paths1 = glob(data_path1)
data_count = len(glob(data_path1))
class_map = {'angry':0, 'disgust':1, 'fearful':2,'happy':3,'neautral':4,'sad':5,'surprised':6}
X = np.zeros((data_count, 278, 40))
y = np.zeros((data_count, ))
for i, path in tqdm(enumerate(sorted(img_paths1))):
    signal,sample_rate = librosa.load(path,sr=22000,mono=True)  # File assumed to be in the same directory

    signal = signal[0:int(2.8 * sample_rate)]  # Keep the first 3.5 seconds
    pre_emphasis = 0.97
    emphasized_signal = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])
    frame_size=0.025
    frame_stride=0.01
    frame_length, frame_step = frame_size * sample_rate, frame_stride * sample_rate  # Convert from seconds to samples
    signal_length = len(emphasized_signal)
    frame_length = int(round(frame_length))
    frame_step = int(round(frame_step))
    num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))  # Make sure that we have at least 1 frame

    pad_signal_length = num_frames * frame_step + frame_length
    z = np.zeros((pad_signal_length - signal_length))
    pad_signal = np.append(emphasized_signal, z) # Pad Signal to make sure that all frames have equal number of samples without truncating any samples from the original signal

    indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T
    frames = pad_signal[indices.astype(np.int32, copy=False)]
    frames *= np.hamming(frame_length)
    NFFT=512
    mag_frames = np.absolute(np.fft.rfft(frames, NFFT))  # Magnitude of the FFT
    pow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))  # Power Spectrum
    
    nfilt = 40
    low_freq_mel = 0
    high_freq_mel = (2595 * np.log10(1 + (sample_rate / 2) / 700))  # Convert Hz to Mel
    mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)  # Equally spaced in Mel scale
    hz_points = (700 * (10**(mel_points / 2595) - 1))  # Convert Mel to Hz
    bin = np.floor((NFFT + 1) * hz_points / sample_rate)

    fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))
    for m in range(1, nfilt + 1):
        f_m_minus = int(bin[m - 1])   # left
        f_m = int(bin[m])             # center
        f_m_plus = int(bin[m + 1])    # right

        for k in range(f_m_minus, f_m):
            fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])
        for k in range(f_m, f_m_plus):
            fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])
    filter_banks = np.dot(pow_frames, fbank.T)
    filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)  # Numerical Stability
    filter_banks = 20 * np.log10(filter_banks)  # dB
    
    num_ceps=12
    mfcc = dct(filter_banks, type=2, axis=1, norm='ortho')[:, 1 : (num_ceps + 1)]
    (nframes, ncoeff) = mfcc.shape
    cep_lifter = 22
    n = np.arange(ncoeff)
    lift = 1 + (cep_lifter / 2) * np.sin(np.pi * n / cep_lifter)
    mfcc *= lift  #*
    filter_banks -= (np.mean(filter_banks, axis=0) + 1e-8)
    X[i] = filter_banks
    print(path)
    cls = path.split('/')[-2]
    print(cls)
    y[i] = class_map[cls]


#In[ ]:

X = np.reshape(X,(data_count,278,40,1))
y_onehot = keras.utils.to_categorical(y, num_classes=7)
np.save('X_test', X)
np.save('y_test',y_onehot)

model.load_weights('ASR1.h5')
X = np.load('X_test.npy')
y_onehot = np.load('y_test.npy')
y_true = np.argmax(y_onehot, axis=-1)
y_pred = np.argmax(model.predict(X), axis=-1)
print(y_true)
print(y_true.shape, y_pred.shape)
target_names = ['angry', 'disgust', 'fearful','happy','neautral','sad','surprised']
print(classification_report(y_true, y_pred, target_names=target_names))
print(confusion_matrix(y_true, y_pred))
np.save('speech_predict',y_pred)

0
